Metadata-Version: 2.4
Name: crash-tournament
Version: 0.1.0
Summary: Adaptive Comparative Judging System for crash report ranking
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: pytest>=8.4.2
Requires-Dist: trueskill>=0.4.5
Requires-Dist: loguru>=0.7.0
Requires-Dist: prettytable>=3.0.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0; extra == "dev"
Requires-Dist: black>=22.0; extra == "dev"
Requires-Dist: isort>=5.0; extra == "dev"
Requires-Dist: mypy>=1.0; extra == "dev"

# Crash Tournament

A TrueSkill-based ranking system for crash reports using adaptive comparative judging. Ranks crashes by exploitability with minimal LLM calls through uncertainty-based active sampling.

## Purpose

This system ranks large sets of crash reports (hundreds to thousands) by likely exploitability. Instead of evaluating each crash individually, it compares small groups of crashes (typically 4 at a time) to build a global ranking using the TrueSkill algorithm. The system performs k-way comparisons but converts them to pairwise TrueSkill updates, trading theoretical rigor for efficiency by assuming transitivity and consistent judge accuracy. By focusing comparisons on crashes with high uncertainty, it minimizes the number of LLM calls needed to achieve confident rankings.

## Approach

We use **TrueSkill** for ranking, but with an important adaptation: while TrueSkill is designed for pairwise comparisons, we perform **k-way ordinal comparisons** (typically 4 crashes at a time) and convert them to sequential pairwise TrueSkill updates. A configurable **Judge** (typically an LLM-based agent like `cursor-agent`) ranks small groups of crashes, and these ordinal results are decomposed into k-1 pairwise comparisons that update TrueSkill ratings (mu/sigma per crash). An **UncertaintySelector** chooses which groups to evaluate next by sampling from crashes with high sigma values, ensuring informative comparisons. 

**k-way judging:** Instead of evaluating crashes strictly pairwise, we present a group of k crashes to the judge and rank them all at once. To update TrueSkill, we naively convert the k-way ranking into k-1 sequential pairwise updates. This sacrifices some theoretical rigor, because converting a k-way ranking into sequential pairwise updates treats correlated outcomes as independent, which can introduce noise and reduce the probabilistic fidelity of the TrueSkill updates.

The efficiency gain comes from an information-per-call tradeoff: one k-way evaluation produces multiple pairwise updates simultaneously. While each derived pairwise update carries (hopefully only) slightly less reliable information than an independent 2-way call, the total information gained per LLM call can (again, hopefully) exceed that of multiple individual pairwise evaluations. Standard TrueSkill treats each pairwise evaluation as the cost unit. In our setting, the expensive resource is the LLM call; k-way judging reduces the number of costly calls while still providing pairwise-equivalent updates for TrueSkill to converge. Even if convergence requires more pairwise updates (due to lower quality pairwise judmgenets to k=2), it should require fewer LLM calls.

For k=2, this reduces to standard TrueSkill with a single pairwise comparison per evaluation.

## Usage

### Quick Start

Rank crashes using the demo script:

```bash
# With cursor-agent judge
uv run python -m crash_tournament.rank_crashes_demo \
    --judge cursor_agent \
    crash1.json crash2.json crash3.json crash4.json

# With simulated judge (for testing)
uv run python -m crash_tournament.rank_crashes_demo \
    --judge sim \
    crash1.json crash2.json crash3.json

# With custom prompt
uv run python -m crash_tournament.rank_crashes_demo \
    --judge cursor_agent \
    --prompt my_prompt.md \
    crash*.json
```

### Full Tournament

Run a complete tournament with the orchestrator:

```bash
uv run python -m crash_tournament \
    --crashes-dir ./crashes \
    --output-dir ./output \
    --judge-type cursor-agent \
    --seed-groups 20

# Stop when uncertainty drops below 0.1
uv run python -m crash_tournament \
    --crashes-dir ./crashes \
    --output-dir ./output \
    --judge-type cursor-agent \
    --uncertainty-threshold 0.1

# Use custom pattern for different file types
uv run python -m crash_tournament \
    --crashes-dir ./crashes \
    --crashes-pattern "crash.json" \
    --output-dir ./output \
    --judge-type simulated
```

### Options

- `--crashes-dir`: Path to crash corpus directory (required)
- `--crashes-pattern`: Pattern for finding crash files (default: `*.json`)
- `--output-dir`: Directory for JSONL/snapshots output (required)
- `--judge-type`: Judge type (`simulated`, `dummy`, `cursor-agent`, `cursor-agent-streaming`)
- `--k`: Group size for comparisons (default: 4)
- `--budget`: Total number of group evaluations (default: auto-computed)
- `--seed-groups`: Initial random groups for coverage (default: 200)
- `--groups-per-round`: Groups per uncertainty round (default: 50)
- `--workers`: Number of worker threads (default: 1)
- `--resume`: Load snapshot and continue from previous run
- `--uncertainty-threshold`: Stop tournament when average uncertainty drops below this threshold (default: run until budget exhausted)
- `--debug`: Enable debug logging

## Architecture

The system uses dependency injection to wire together swappable components:

### Core Interfaces

**`CrashFetcher`** — Abstract interface for crash data sources (not limited to files)
- `list_crashes() -> Iterable[Crash]`: Returns all crashes
- `get_crash(crash_id) -> Crash`: Fetch specific crash
- Implementation: `DirectoryCrashFetcher` (scans directory for crash files)

**`Judge`** — Compares a group of crashes and returns ranked order
- `evaluate_group(crashes: Sequence[Crash], *, grading: bool = False) -> OrdinalResult`: Returns ordered crash IDs
- Implementations:
  - `CursorAgentJudge`: Wraps `cursor-agent` CLI (blocking, single JSON response)
    - **Expected JSON schema:** `{"ranked_ids": ["crash_1", "crash_2", ...], "rationale": "...", ...}`
    - Parser extracts `ranked_ids` for `OrdinalResult.ordered_ids`, stores full JSON in `parsed_result`
  - `CursorAgentStreamingJudge`: Streaming JSONL variant (--output-format=stream-json)
  - `SimulatedJudge`: Synthetic judge with configurable noise for testing
  - `DummyJudge`: Deterministic/random responses for testing

**`Storage`** — Persists observations and system snapshots
- `persist_ordinal(res: OrdinalResult)`: Append observation
- `load_observations() -> Iterable[OrdinalResult]`: Load all observations
- `save_snapshot(state: dict)`: Save system state (idempotent)
- `load_snapshot() -> Optional[dict]`: Restore state
- Implementation: `JSONLStorage` (JSONL for observations, JSON for snapshots)
- **Reproducibility:** Each observation includes `timestamp` and `raw_output`; no deduplication (groups may be re-evaluated)

**`Ranker`** — Maintains TrueSkill ratings for all crashes
- `update_with_ordinal(res: OrdinalResult, weight: float)`: Update ratings from comparison
- `get_score(crash_id) -> float`: Get mu (skill estimate)
- `get_uncertainty(crash_id) -> float`: Get sigma (uncertainty)
- `snapshot() -> dict` / `load_snapshot(state: dict)`: Serialize/deserialize state
- Implementation: `TrueSkillRanker` (k-way → k-1 pairwise conversions, configurable weight parameter, default 1/(k-1))

**Important:** TrueSkill is fundamentally a pairwise rating system, but we perform k-way comparisons for efficiency. Each k-way comparison is converted to k-1 sequential pairwise TrueSkill updates (e.g., a 4-way comparison becomes 3 pairwise updates: 1st vs 2nd, 2nd vs 3rd, 3rd vs 4th). The weight parameter (default 1/(k-1)) prevents overconfidence from treating k-1 pairwise updates as k-1 independent comparisons.

**Tradeoffs:** This approach assumes transitivity and consistent judge accuracy across the k-way comparison, which may introduce noise. The benefit is fewer LLM calls for convergence, but at the cost of potential ranking errors from these assumptions.

**Special case k=2:** When k=2, the system simplifies to true TrueSkill with exactly one pairwise comparison per evaluation, making it equivalent to standard TrueSkill usage.

**`Selector`** — Decides which crash groups to evaluate next
- `next_groups(all_crash_ids: Sequence[str], k: int, budget: int) -> Sequence[Sequence[str]]`: Generate groups
- Implementation: `UncertaintySelector` (samples high-sigma crashes, fills groups with nearby-mu crashes)

**`Orchestrator`** — Main control loop
- Wires all components via dependency injection
- Runs seed phase (random groups for initial coverage)
- Executes uncertainty rounds (adaptive sampling loop)
- Manages thread pool for concurrent judge calls
- Uses random group generation during seed phase, then switches to UncertaintySelector for adaptive phase
- **Thread safety:** Ranker updates and Storage writes are serialized; only Judge.evaluate_group calls run in parallel
- Handles snapshotting and restart logic
- Enforces budget and stopping conditions

### Data Flow

```
CrashFetcher → Orchestrator → Selector → Judge → Storage
                     ↓                       ↓
                  Ranker ←──────────── persisted observations
```

1. **Orchestrator** gets crash IDs from **CrashFetcher**
2. **Selector** uses **Ranker** uncertainty (sigma) to choose groups
3. **Orchestrator** calls **Judge** (in thread pool) to rank each group
4. **Judge** results persisted via **Storage** and fed to **Ranker**
5. **Ranker** updates TrueSkill ratings (mu/sigma)
6. Repeat until budget exhausted or convergence

## Seeding and Adaptive Phases

The tournament uses a two-phase approach to bootstrap TrueSkill efficiently:

### Seed Phase

Initial random comparisons establish baseline TrueSkill structure. Without seeding, all crashes start with identical uncertainty (σ), making adaptive selection initially no better than random.

- **Config:** `seed_groups` parameter (default: 200)
- **Behavior:** Generates random k-sized groups, evaluates via judge, updates TrueSkill
- **Coverage:** With 1000 crashes, k=4, seed_groups=200 → each crash appears ~0.8 times
- **Result:** σ and μ values diverge, enabling meaningful uncertainty-based selection

### Adaptive Phase

After seeding, UncertaintySelector focuses evaluations on high-uncertainty crashes to efficiently resolve ranking ambiguity.

- **Selector:** Uses ranker to find crashes with high σ
- **Grouping:** Pairs uncertain crashes with nearby-μ crashes for informative comparisons
- **Iteration:** Continues until budget exhausted or convergence
- **Snapshots:** Written after each round for idempotency

**Flow:** Seed (random) → Update TrueSkill → Adaptive (uncertainty-based) → Update TrueSkill → Repeat

**Orchestrator control flow:** The orchestrator considers seeding complete after evaluating exactly `seed_groups` random groups (CLI arg `--seed-groups`, default 200). It then switches to uncertainty-based selection, running adaptive rounds until `evaluated_groups >= budget` (CLI arg `--budget`). On resume from snapshot, if `evaluated_groups < seed_groups`, seeding continues; otherwise adaptive phase begins.

**Key CLI arguments:**
- `--seed-groups`: Random groups in seed phase (default: 200)
- `--budget`: Total evaluations across seed + adaptive phases (default: auto-computed)
- `--groups-per-round`: Groups per adaptive round (default: 50)
- `--k`: Crashes per group (default: 4, minimum 2)
- `--workers`: Number of worker threads (default: 1)
- `--judge-type`: Judge implementation (default: simulated)
- `--resume`: Load snapshot and continue from previous run

**Note:** If fewer crashes are available than `k`, the system uses all available crashes (effectively reducing group size). For meaningful tournaments, ensure you have significantly more crashes than `k`.

## UncertaintySelector Algorithm

Adaptive sampling algorithm that minimizes evaluations while maximizing information gain.

**Parameters:**
- `K_uncertain`: Top N uncertain crashes to consider (default: 5*k)
- `delta_mu`: Max μ difference for "nearby" crashes (default: 1.0)
- `max_evals_per_crash`: Optional evaluation limit per crash

**Per-round algorithm:**
1. Query ranker for all σ values, sort descending, take top K_uncertain
2. For each group: select highest-σ crash not over-evaluated, find crashes with |μ - target_μ| ≤ delta_mu, fill group with k-1 nearby crashes
3. If insufficient nearby crashes, fill with random available crashes
4. Track groups as normalized tuples to avoid duplicates (retry up to 3 times)
5. Update eval_counts to respect max_evals_per_crash

**Rationale:** Comparing similar-scored (nearby μ) but uncertain (high σ) crashes efficiently resolves ranking ambiguity.

### Key Design Decisions

- **Crashes as black boxes**: The tournament system treats crashes as file paths without parsing content. Judges read file content directly as needed.
- **Synchronous interfaces**: All components use synchronous methods. Concurrency is handled by the orchestrator's thread pool.
- **TrueSkill adaptation**: k-way comparisons are converted to k-1 pairwise TrueSkill updates with weight=1/(k-1) to avoid overconfidence. This trades theoretical rigor for efficiency by assuming transitivity and consistent judge accuracy across k-way comparisons.
- **Idempotency**: System can resume from snapshots without re-evaluating groups.
- **Pluggable components**: All interfaces are abstract; swap implementations without changing orchestrator logic.

## Multithreaded Architecture

The orchestrator uses `ThreadPoolExecutor` to parallelize judge evaluations while keeping state updates thread-safe.

### Threading Model

**What runs in parallel:**
- Judge evaluations (`judge.evaluate_group()`) run concurrently in worker threads
- Each worker handles one group at a time, reading crash files and invoking the judge
- With judges taking 1+ minutes per evaluation, workers do the heavy lifting

**What runs sequentially:**
- Result processing (storage writes, ranker updates) happens in the main thread
- Failure tracking and abort checks run serially after each evaluation completes
- Snapshots are saved between rounds, never during parallel execution

**How it works:**
1. Main thread submits all groups in a round to the thread pool
2. Workers execute judge calls concurrently (1+ minute each)
3. Main thread blocks waiting for next result using `as_completed()` 
4. When a worker finishes, main thread wakes up, processes that result (~milliseconds)
5. Result is persisted to storage and updates the ranker
6. Main thread blocks again waiting for next completion
7. After all evaluations complete, a snapshot is saved

**Performance:** With `--workers 4` and 1-minute judge calls, workers spend ~4 minutes of wall-clock time evaluating while main thread spends most time blocked waiting, briefly waking to process each result. Parallelism significantly reduces total runtime (4 groups in ~1 minute instead of ~4 minutes sequential).

**Thread safety:** Workers are read-only (only read crash files), while all writes (storage, ranker, counters) happen serially in the main thread. No locks needed because only one thread writes.

### Exception Handling

**When a worker fails:**
- Exception is captured and logged by the main thread when it retrieves that result
- Failed evaluation is skipped (not persisted, doesn't update ranker)
- Failure counters are incremented
- Abort conditions are checked (100% of first 4, or >20% after 50 calls)
- If abort triggered: remaining work is cancelled and tournament stops
- If below threshold: tournament continues with remaining evaluations

**Multiple simultaneous failures:**
- Main thread processes failures one-by-one as they complete
- First failure to exceed threshold triggers abort
- In-flight workers may complete but results are discarded

### Shutdown

**Normal completion:** All work finishes, snapshot saved with final state

**Abort (threshold exceeded):** Tournament stops immediately, thread pool shuts down, last snapshot reflects pre-abort state

**Ctrl-C:** `KeyboardInterrupt` triggers clean shutdown, snapshot reflects last completed round

### Configuration

Set `--workers` based on your judge type: use 1 for sequential execution (default, safest), or higher values (2-8) for parallel execution with fast judges.

## Data Models

**`Crash`** — Black-box crash representation
- `crash_id: str` — Unique identifier
- `file_path: str` — Path to crash file (judge reads this)
- `timestamp: float` — Creation time

**`OrdinalResult`** — Judge evaluation output
- `ordered_ids: List[str]` — Crash IDs ranked most→least exploitable
- `raw_output: str` — Raw judge output for audit
- `parsed_result: dict` — Structured data from judge
- `timestamp: float` — Evaluation time
- `judge_id: str` — Judge identifier
- `group_size: int` — Number of crashes evaluated

**`GradedResult`** — Future graded evaluation support
- `grades: Dict[str, float]` — Crash ID → numeric score
- `raw_output: str`, `parsed_result: dict`, `timestamp: float`, `judge_id: str`, `group_size: int`

## Directory Structure

```
crash_tournament/
├── crash_tournament/
│   ├── __main__.py              # CLI entry point for full tournament
│   ├── models.py                # Data models: Crash, OrdinalResult, GradedResult
│   ├── interfaces.py            # Abstract base classes for all components
│   ├── orchestrator.py          # Main control loop (seed + adaptive phases)
│   ├── rank_crashes_demo.py    # Simple demo script for quick testing
│   ├── fetchers/
│   │   └── directory_fetcher.py # Scans directory for crash files
│   ├── judges/
│   │   ├── cursor_agent_judge.py          # Wraps cursor-agent CLI
│   │   ├── cursor_agent_streaming_judge.py # Streaming JSONL variant
│   │   └── sim_judge.py                    # Simulated judge for testing
│   ├── rankers/
│   │   └── trueskill_ranker.py  # TrueSkill with k-way→pairwise conversion
│   ├── group_selectors/
│   │   └── uncertainty_selector.py # Uncertainty-based adaptive sampling
│   ├── storage/
│   │   └── jsonl_storage.py     # JSONL observations + JSON snapshots
│   └── prompts/
│       └── ordinal_judge.md     # Default prompt template
└── tests/                       # Unit and integration tests
```

## Testing

```bash
# Run all tests
uv run python -m pytest tests/ -v

# Run specific test suites
uv run python -m pytest tests/test_trueskill_ranker.py -v
uv run python -m pytest tests/test_uncertainty_selector.py -v
uv run python -m pytest tests/test_integration.py -v
```

**Test coverage:**
- ✓ Unit tests: TrueSkill ranker, uncertainty selector, JSONL storage
- ✓ Integration tests: End-to-end tournament, uncertainty selector integration
- ✗ **Missing:** Seed→adaptive transition, snapshot resume mid-tournament, orchestrator config validation

## Ranked Directory and Symlinks

The system creates a `ranked/` directory in the output folder containing symbolic links to crash files, ordered by exploitability ranking. This provides easy access to the most exploitable crashes without parsing JSON files.

**When created:**
- **At tournament completion:** Final ranked directory created with all crashes
- **At milestones:** Directory recreated every 50 evaluations during long tournaments
- **On resume:** Directory recreated when resuming from snapshots

**Symlink format:**
```
output/ranked/
├── 1_crash_001 -> /path/to/crash_001.json
├── 2_crash_045 -> /path/to/crash_045.json
├── 3_crash_123 -> /path/to/crash_123.json
└── ...
```

**Symlink naming:** `{rank}_{crash_id}` where rank is 1-based position in final ranking

**Usage:**
```bash
# View top 5 most exploitable crashes
ls -la output/ranked/ | head -6

# Open the #1 ranked crash
cat output/ranked/1_*

# Find crashes by rank range
ls output/ranked/ | grep "^[1-5]_"
```

**Directory lifecycle:**
- Cleared and recreated at each milestone/update
- Contains absolute symlinks to original crash files
- Preserves original file structure and naming

## Snapshot Format and Idempotency

Snapshots enable flexible resume scenarios without re-evaluating completed groups.

**Schema** (`snapshot.json`):
```json
{
  "ranker_state": {
    "crash_001": {"mu": 28.5, "sigma": 6.2},
    "crash_002": {"mu": 22.1, "sigma": 7.8}
  },
  "runtime_state": {
    "evaluated_groups": 150,
    "current_round": 5
  }
}
```

**Resume scenarios:**
- **Ctrl-C interruption:** Resume from last completed round
- **System failure:** Resume from last snapshot
- **Budget extension:** Continue with larger budget after previous completion
- **Parameter changes:** Resume with different judge/workers while preserving rankings
  - ⚠️ **Warning:** Changing judge type or parameters mid-tournament destabilizes TrueSkill ratings and may produce inconsistent rankings

**Resume behavior:**
1. Load snapshot and observations on start
2. Restore all TrueSkill μ/σ values and evaluation counts
3. Continue from current state with new budget/parameters
4. Already-evaluated groups never re-evaluated

**Idempotency:** Identical final rankings whether run continuously or after restart (for deterministic judges).

**Note:** Snapshots written after each uncertainty round (all groups in that round). Mid-round interruption loses only that round's work.

## Error Handling and Limitations

**Judge Error Tolerance:**
The orchestrator tolerates up to 20% judge failures but aborts if 100% of the first 4 calls fail (judge broken) or if failure rate exceeds 20% after 50+ calls (systematic issues). This applies to all judge types (LLM-based, simulated, dummy, etc.).

## Dependencies

- Python 3.10+
- Package management via `uv`

```bash
uv sync  # Install all dependencies
```

